use std::{
    collections::{BTreeMap, HashMap},
    net::SocketAddr,
    sync::Arc,
    time::Duration,
};

use bytes::{BufMut, Bytes, BytesMut};
use tokio::{
    io::{AsyncReadExt, AsyncWriteExt},
    net::{TcpListener, TcpStream},
    time::{interval, Instant},
};
use tracing::{debug, error, info, instrument, warn};

use crate::{
    connection_pool::{
        handle_incoming_message, ChannelId, ConnectionState, LockFreeConnection,
        LockFreeStreamHandle,
    },
    registry::{GossipRegistry, GossipResult, GossipTask, RegistryMessage, RegistryStats},
    GossipConfig, GossipError, RegistrationPriority, RemoteActorLocation, Result,
};

/// Per-connection streaming state for managing partial streams
#[derive(Debug)]
struct StreamingState {
    active_streams: HashMap<u64, InProgressStream>,
    max_concurrent_streams: usize,
}

/// A stream that is currently being assembled
#[derive(Debug)]
struct InProgressStream {
    stream_id: u64,
    total_size: u64,
    type_hash: u32,
    actor_id: u64,
    chunks: BTreeMap<u32, Bytes>, // chunk_index -> chunk_data
    received_size: usize,
}

impl StreamingState {
    fn new() -> Self {
        Self {
            active_streams: HashMap::new(),
            max_concurrent_streams: 16, // Reasonable limit
        }
    }

    fn start_stream(&mut self, header: crate::StreamHeader) -> Result<()> {
        if self.active_streams.len() >= self.max_concurrent_streams {
            return Err(GossipError::Network(std::io::Error::new(
                std::io::ErrorKind::ResourceBusy,
                "Too many concurrent streams",
            )));
        }

        let stream = InProgressStream {
            stream_id: header.stream_id,
            total_size: header.total_size,
            type_hash: header.type_hash,
            actor_id: header.actor_id,
            chunks: BTreeMap::new(),
            received_size: 0,
        };

        self.active_streams.insert(header.stream_id, stream);
        Ok(())
    }

    fn add_chunk(
        &mut self,
        header: crate::StreamHeader,
        chunk_data: Vec<u8>,
    ) -> Result<Option<Vec<u8>>> {
        let stream = self
            .active_streams
            .get_mut(&header.stream_id)
            .ok_or_else(|| {
                GossipError::Network(std::io::Error::new(
                    std::io::ErrorKind::InvalidData,
                    format!("Received chunk for unknown stream_id={}", header.stream_id),
                ))
            })?;

        // Store chunk
        let chunk_bytes = Bytes::from(chunk_data);
        stream.received_size += chunk_bytes.len();
        stream.chunks.insert(header.chunk_index, chunk_bytes);

        // Check if we have all chunks (when total matches expected size)
        if stream.received_size >= stream.total_size as usize {
            self.assemble_complete_message(header.stream_id)
        } else {
            Ok(None)
        }
    }

    fn finalize_stream(&mut self, stream_id: u64) -> Result<Option<Vec<u8>>> {
        // StreamEnd received - assemble the message
        self.assemble_complete_message(stream_id)
    }

    fn assemble_complete_message(&mut self, stream_id: u64) -> Result<Option<Vec<u8>>> {
        let stream = self.active_streams.remove(&stream_id).ok_or_else(|| {
            GossipError::Network(std::io::Error::new(
                std::io::ErrorKind::InvalidData,
                format!("Cannot finalize unknown stream_id={}", stream_id),
            ))
        })?;

        // Assemble chunks in order
        let mut complete_data = BytesMut::with_capacity(stream.total_size as usize);
        for (_chunk_index, chunk_data) in stream.chunks {
            complete_data.put_slice(&chunk_data);
        }

        info!(
            "‚úÖ STREAMING: Assembled complete message for stream_id={} ({} bytes for actor={})",
            stream_id,
            complete_data.len(),
            stream.actor_id
        );

        Ok(Some(complete_data.to_vec()))
    }
}

/// Main API for the gossip registry with vector clocks and separated locks
pub struct GossipRegistryHandle {
    pub registry: Arc<GossipRegistry>,
    _server_handle: tokio::task::JoinHandle<()>,
    _timer_handle: tokio::task::JoinHandle<()>,
    _monitor_handle: Option<tokio::task::JoinHandle<()>>,
}

impl GossipRegistryHandle {
    /// Create and start a new gossip registry with TLS encryption
    ///
    /// This creates a secure gossip registry that uses TLS 1.3 for all connections.
    /// The secret_key is used to generate the node's identity certificate.
    pub async fn new_with_tls(
        bind_addr: SocketAddr,
        secret_key: crate::SecretKey,
        config: Option<GossipConfig>,
    ) -> Result<Self> {
        let config = config.unwrap_or_default();

        // Create the TCP listener first to get the actual bound address
        let listener = TcpListener::bind(bind_addr).await?;
        let actual_bind_addr = listener.local_addr()?;

        // Create registry with TLS enabled
        let mut registry = GossipRegistry::new(actual_bind_addr, config.clone());
        registry.enable_tls(secret_key)?;

        let registry = Arc::new(registry);

        // Set the registry reference in the connection pool
        {
            let mut pool = registry.connection_pool.lock().await;
            pool.set_registry(registry.clone());
        }

        // Start the server with the existing listener
        let server_registry = registry.clone();
        let server_handle = tokio::spawn(async move {
            if let Err(err) = start_gossip_server_with_listener(server_registry, listener).await {
                error!(error = %err, "TLS server error");
            }
        });

        // Start the gossip timer
        let timer_registry = registry.clone();
        let timer_handle = tokio::spawn(async move {
            start_gossip_timer(timer_registry).await;
        });

        // Connection monitoring is now done in the gossip timer
        let monitor_handle = None;

        info!(bind_addr = %actual_bind_addr, "TLS-enabled gossip registry started");

        Ok(Self {
            registry,
            _server_handle: server_handle,
            _timer_handle: timer_handle,
            _monitor_handle: monitor_handle,
        })
    }

    /// Create and start a new gossip registry
    ///
    /// Note: The peers parameter is deprecated. Always pass an empty Vec and use
    /// add_peer() after creation to ensure proper peer ID assignment.
    #[instrument(skip(peers, config))]
    pub async fn new(
        bind_addr: SocketAddr,
        peers: Vec<SocketAddr>,
        config: Option<GossipConfig>,
    ) -> Result<Self> {
        let config = config.unwrap_or_default();

        // Create the TCP listener first to get the actual bound address
        let listener = TcpListener::bind(bind_addr).await?;
        let actual_bind_addr = listener.local_addr()?;

        let registry = GossipRegistry::new(actual_bind_addr, config.clone());

        // Note: Peers should be added manually with add_peer() after creation
        // to ensure proper peer ID assignment. The peers parameter is kept
        // for backward compatibility but is now deprecated.
        if !peers.is_empty() {
            warn!(
                "Passing peer addresses to GossipRegistryHandle::new() is deprecated. \
                 Use handle.add_peer() after creation to set proper peer IDs."
            );
            // For backward compatibility, we still configure them but with a warning
            for (i, peer_addr) in peers.into_iter().enumerate() {
                let node_name = format!("peer_{}", i);
                warn!(
                    "Auto-generating peer name '{}' for {} - this is deprecated",
                    node_name, peer_addr
                );
                registry.configure_peer(node_name, peer_addr).await;
            }
        }

        let registry = Arc::new(registry);

        // Set the registry reference in the connection pool
        {
            let mut pool = registry.connection_pool.lock().await;
            pool.set_registry(registry.clone());
        }

        // Start the server with the existing listener
        let server_registry = registry.clone();
        let server_handle = tokio::spawn(async move {
            if let Err(err) = start_gossip_server_with_listener(server_registry, listener).await {
                error!(error = %err, "server error");
            }
        });

        // Start the gossip timer
        debug!("üöÄ About to spawn gossip timer task");
        let timer_registry = registry.clone();
        let timer_handle = tokio::spawn(async move {
            debug!("üöÄ Gossip timer task spawned, calling start_gossip_timer");
            start_gossip_timer(timer_registry).await;
            warn!("‚ö†Ô∏è Gossip timer exited unexpectedly!");
        });
        debug!("üöÄ Gossip timer spawn complete");

        // Connection monitoring is now done in the gossip timer
        let monitor_handle = None;

        info!(bind_addr = %actual_bind_addr, "gossip registry started");

        Ok(Self {
            registry,
            _server_handle: server_handle,
            _timer_handle: timer_handle,
            _monitor_handle: monitor_handle,
        })
    }

    /// Register a local actor
    pub async fn register(&self, name: String, address: SocketAddr) -> Result<()> {
        let location = RemoteActorLocation::new_with_peer(address, self.registry.peer_id.clone());
        self.registry.register_actor(name, location).await
    }

    /// Register a local actor with metadata
    pub async fn register_with_metadata(
        &self,
        name: String,
        address: SocketAddr,
        metadata: Vec<u8>,
    ) -> Result<()> {
        let location = RemoteActorLocation::new_with_metadata(
            address,
            self.registry.peer_id.clone(),
            metadata,
        );
        self.registry.register_actor(name, location).await
    }

    /// Register a local actor with high priority (faster propagation)
    pub async fn register_urgent(
        &self,
        name: String,
        address: SocketAddr,
        priority: RegistrationPriority,
    ) -> Result<()> {
        let mut location =
            RemoteActorLocation::new_with_peer(address, self.registry.peer_id.clone());
        location.priority = priority;
        self.registry
            .register_actor_with_priority(name, location, priority)
            .await
    }

    /// Register a local actor with specified priority
    pub async fn register_with_priority(
        &self,
        name: String,
        address: SocketAddr,
        priority: RegistrationPriority,
    ) -> Result<()> {
        let mut location =
            RemoteActorLocation::new_with_peer(address, self.registry.peer_id.clone());
        location.priority = priority;
        self.registry
            .register_actor_with_priority(name, location, priority)
            .await
    }

    /// Unregister a local actor
    pub async fn unregister(&self, name: &str) -> Result<Option<RemoteActorLocation>> {
        self.registry.unregister_actor(name).await
    }

    /// Lookup an actor (now much faster - read-only lock)
    pub async fn lookup(&self, name: &str) -> Option<RemoteActorLocation> {
        self.registry.lookup_actor(name).await
    }

    /// Get registry statistics including vector clock metrics
    pub async fn stats(&self) -> RegistryStats {
        self.registry.get_stats().await
    }

    /// Add a peer to the gossip network
    pub async fn add_peer(&self, peer_id: &crate::PeerId) -> crate::Peer {
        // Pre-configure the peer as allowed (address will be set when connect() is called)
        {
            let pool = self.registry.connection_pool.lock().await;
            // Use a placeholder address - will be updated when connect() is called
            pool.peer_id_to_addr
                .insert(peer_id.clone(), "0.0.0.0:0".parse().unwrap());
        }

        crate::Peer {
            peer_id: peer_id.clone(),
            registry: self.registry.clone(),
        }
    }

    /// Get a connection handle for direct communication (reuses existing pool connections)
    pub async fn get_connection(
        &self,
        addr: SocketAddr,
    ) -> Result<crate::connection_pool::ConnectionHandle> {
        self.registry.get_connection(addr).await
    }

    /// Shutdown the registry
    pub async fn shutdown(&self) {
        self.registry.shutdown().await;

        // Cancel background tasks (they will terminate when they detect shutdown)
        self._server_handle.abort();
        self._timer_handle.abort();
        if let Some(monitor_handle) = &self._monitor_handle {
            monitor_handle.abort();
        }

        // No artificial delays - connections will close immediately
    }
}

/// Start the gossip registry server with an existing listener
#[instrument(skip(registry, listener))]
async fn start_gossip_server_with_listener(
    registry: Arc<GossipRegistry>,
    listener: TcpListener,
) -> Result<()> {
    let bind_addr = registry.bind_addr;
    info!(bind_addr = %bind_addr, "gossip server started");

    loop {
        match listener.accept().await {
            Ok((stream, peer_addr)) => {
                info!(peer_addr = %peer_addr, "üì• ACCEPTED incoming connection");
                // Set TCP_NODELAY for low-latency communication
                let _ = stream.set_nodelay(true);

                let registry_clone = registry.clone();
                tokio::spawn(async move {
                    handle_connection(stream, peer_addr, registry_clone).await;
                });
            }
            Err(err) => {
                error!(error = %err, "failed to accept connection");
            }
        }
    }
}

/// Start the gossip timer with vector clock support
#[instrument(skip(registry))]
async fn start_gossip_timer(registry: Arc<GossipRegistry>) {
    debug!("start_gossip_timer function called");

    let gossip_interval = registry.config.gossip_interval;
    let cleanup_interval = registry.config.cleanup_interval;
    let vector_clock_gc_interval = registry.config.vector_clock_gc_frequency;

    let jitter = Duration::from_millis(rand::random::<u64>() % 1000);
    let mut next_gossip_tick = Instant::now() + gossip_interval + jitter;
    let mut cleanup_timer = interval(cleanup_interval);
    let mut vector_clock_gc_timer = interval(vector_clock_gc_interval);

    debug!(
        gossip_interval_ms = gossip_interval.as_millis(),
        cleanup_interval_secs = cleanup_interval.as_secs(),
        vector_clock_gc_interval_secs = vector_clock_gc_interval.as_secs(),
        "gossip timer started with non-blocking I/O"
    );

    loop {
        tokio::select! {
            _ = tokio::time::sleep_until(next_gossip_tick) => {
                let jitter = Duration::from_millis(rand::random::<u64>() % 1000);
                next_gossip_tick += gossip_interval + jitter;

                // Step 1: Prepare gossip tasks while holding the lock briefly
                let tasks = {
                    if registry.is_shutdown().await {
                        break;
                    }
                    match registry.prepare_gossip_round().await {
                        Ok(tasks) => tasks,
                        Err(err) => {
                            error!(error = %err, "failed to prepare gossip round");
                            continue;
                        }
                    }
                };

                if tasks.is_empty() {
                    continue;
                }

                // Step 2: Execute all gossip tasks WITHOUT holding the registry lock
                // Use zero-copy optimized sending for each individual gossip message
                let results = {
                    let mut futures = Vec::new();

                    for task in tasks {
                        let registry_clone = registry.clone();
                        let peer_addr = task.peer_addr;
                        let sent_sequence = task.current_sequence;
                        let future = tokio::spawn(async move {
                            // Send the message using zero-copy persistent connections
                            let outcome = send_gossip_message_zero_copy(task, registry_clone).await;
                            GossipResult {
                                peer_addr,
                                sent_sequence,
                                outcome: outcome.map(|_| None),
                            }
                        });
                        futures.push(future);
                    }

                    // Wait for all gossip operations to complete concurrently
                    let mut results = Vec::new();
                    for future in futures {
                        match future.await {
                            Ok(result) => results.push(result),
                            Err(err) => {
                                error!(error = %err, "gossip task panicked");
                            }
                        }
                    }
                    results
                };

                // Step 3: Apply results while holding the lock briefly
                {
                    if !registry.is_shutdown().await {
                        registry.apply_gossip_results(results).await;
                    }
                }
            }
            _ = cleanup_timer.tick() => {
                if registry.is_shutdown().await {
                    break;
                }
                registry.cleanup_stale_actors().await;
                // Also check for consensus timeouts
                registry.check_peer_consensus().await;
                // Clean up peers that have been dead for too long
                registry.cleanup_dead_peers().await;
            }
            _ = vector_clock_gc_timer.tick() => {
                if registry.is_shutdown().await {
                    break;
                }
                // Run vector clock garbage collection
                registry.run_vector_clock_gc().await;
            }
        }
    }

    debug!("gossip timer stopped");
}

/// Handle incoming TCP connections - immediately set up bidirectional communication
#[instrument(skip(stream, registry), fields(peer = %peer_addr))]
async fn handle_connection(
    stream: TcpStream,
    peer_addr: SocketAddr,
    registry: Arc<GossipRegistry>,
) {
    debug!("üîå HANDLE_CONNECTION: Starting to handle new incoming connection");

    // Check if TLS is enabled
    if let Some(tls_config) = &registry.tls_config {
        // TLS is enabled - perform TLS handshake
        info!(
            "üîê TLS ENABLED: Performing TLS handshake with peer {}",
            peer_addr
        );

        let acceptor = tls_config.acceptor();
        match acceptor.accept(stream).await {
            Ok(tls_stream) => {
                info!("‚úÖ TLS handshake successful with peer {}", peer_addr);
                handle_tls_connection(tls_stream, peer_addr, registry).await;
            }
            Err(e) => {
                error!(error = %e, peer = %peer_addr, "‚ùå TLS handshake failed - rejecting connection");
                // Connection failed, don't continue
                // This is correct - we should NOT fall back to plain TCP if TLS is enabled
            }
        }
    } else {
        // No TLS - panic for now to ensure we're using TLS
        panic!("‚ö†Ô∏è TLS DISABLED: Server attempted to accept plain TCP connection from {}. TLS is required!", peer_addr);

        // COMMENTED OUT: Non-TLS TCP path - migrating to TLS-only
        // // Split stream for direct TCP access
        // let (reader, writer) = stream.into_split();

        // // Get registry reference for the handler
        // let registry_weak = Some(Arc::downgrade(&registry));

        // // Start the incoming persistent connection handler immediately
        // // It will handle ALL messages including the first one
        // tokio::spawn(async move {
        //     debug!(peer = %peer_addr, "HANDLE.RS: Starting incoming direct TCP connection handler");
        //     let sender_node_id = handle_incoming_connection_direct_tcp(
        //         reader,
        //         writer,
        //         peer_addr,
        //         registry.clone(),
        //         registry_weak,
        //     )
        //     .await;
        //     debug!(peer = %peer_addr, "HANDLE.RS: Incoming direct TCP connection handler exited");
        //
        //     // Handle peer failure when connection is lost
        //     if let Some(failed_node_id) = sender_node_id {
        //         debug!(node_id = %failed_node_id, "HANDLE.RS: Triggering peer failure handling for node");
        //         if let Err(e) = registry.handle_peer_connection_failure_by_node_id(&failed_node_id).await {
        //             warn!(error = %e, node_id = %failed_node_id, "HANDLE.RS: Failed to handle peer connection failure");
        //         }
        //     } else {
        //         warn!(peer = %peer_addr, "HANDLE.RS: Cannot handle peer failure - sender node ID unknown");
        //     }
        // });
    }
}

/// Handle an incoming TLS connection
async fn handle_tls_connection(
    tls_stream: tokio_rustls::server::TlsStream<TcpStream>,
    peer_addr: SocketAddr,
    registry: Arc<GossipRegistry>,
) {
    // Split the TLS stream
    let (reader, writer) = tokio::io::split(tls_stream);

    // Get registry reference for the handler
    let registry_weak = Some(Arc::downgrade(&registry));

    // Start the incoming persistent connection handler
    tokio::spawn(async move {
        debug!(peer = %peer_addr, "HANDLE.RS: Starting incoming TLS connection handler");
        let sender_node_id = handle_incoming_connection_tls(
            reader,
            writer,
            peer_addr,
            registry.clone(),
            registry_weak,
        )
        .await;
        debug!(peer = %peer_addr, "HANDLE.RS: Incoming TLS connection handler exited");

        // Handle peer failure when connection is lost
        if let Some(failed_node_id) = sender_node_id {
            debug!(node_id = %failed_node_id, "HANDLE.RS: Triggering peer failure handling for node");
            if let Err(e) = registry
                .handle_peer_connection_failure_by_node_id(&failed_node_id)
                .await
            {
                warn!(error = %e, node_id = %failed_node_id, "HANDLE.RS: Failed to handle peer connection failure");
            }
        } else {
            warn!(peer = %peer_addr, "HANDLE.RS: Cannot handle peer failure - sender node ID unknown");
        }
    });
}

/// Handle an incoming TLS connection - processes all messages over encrypted stream
async fn handle_incoming_connection_tls<R, W>(
    mut reader: R,
    writer: W,
    peer_addr: SocketAddr,
    registry: Arc<GossipRegistry>,
    _registry_weak: Option<std::sync::Weak<GossipRegistry>>,
) -> Option<String>
where
    R: AsyncReadExt + Unpin + Send + 'static,
    W: AsyncWriteExt + Unpin + Send + 'static,
{
    let max_message_size = registry.config.max_message_size;

    // Initialize streaming state for this connection
    let mut streaming_state = StreamingState::new();

    // First, read the initial message to identify the sender
    let msg_result = read_message_from_tls_reader(&mut reader, max_message_size).await;

    let (sender_node_id, initial_correlation_id) = match &msg_result {
        Ok(MessageReadResult::Gossip(msg, correlation_id)) => {
            let node_id = match msg {
                RegistryMessage::DeltaGossip { delta } => delta.sender_peer_id.as_str().to_string(),
                RegistryMessage::FullSync { sender_peer_id, .. } => {
                    sender_peer_id.as_str().to_string()
                }
                RegistryMessage::FullSyncRequest { sender_peer_id, .. } => {
                    sender_peer_id.as_str().to_string()
                }
                RegistryMessage::FullSyncResponse { sender_peer_id, .. } => {
                    sender_peer_id.as_str().to_string()
                }
                RegistryMessage::DeltaGossipResponse { delta } => {
                    delta.sender_peer_id.as_str().to_string()
                }
                RegistryMessage::PeerHealthQuery { sender, .. } => sender.as_str().to_string(),
                RegistryMessage::PeerHealthReport { reporter, .. } => reporter.as_str().to_string(),
                RegistryMessage::ImmediateAck { .. } => {
                    warn!("Received ImmediateAck as first message - cannot identify sender");
                    return None;
                }
                RegistryMessage::ActorMessage { .. } => {
                    // For ActorMessage, we can't determine sender from the message
                    // But if it has a correlation_id, it's an Ask and we should handle it
                    if correlation_id.is_some() {
                        debug!("Received ActorMessage with Ask envelope as first message");
                        // We'll use a placeholder sender ID for now
                        "ask_sender".to_string()
                    } else {
                        warn!("Received ActorMessage as first message - cannot identify sender");
                        return None;
                    }
                }
            };
            (node_id, *correlation_id)
        }
        Ok(MessageReadResult::Actor { actor_id, .. }) => {
            // For actor messages received as the first message, we can't determine the sender
            // Use a placeholder identifier
            (format!("actor_sender_{}", actor_id), None)
        }
        Ok(MessageReadResult::Streaming { stream_header, .. }) => {
            // For streaming messages received as the first message, use the actor ID
            (format!("stream_sender_{}", stream_header.actor_id), None)
        }
        Err(e) => {
            warn!(error = %e, "Failed to read initial message from TLS stream");
            return None;
        }
    };

    debug!(peer_addr = %peer_addr, node_id = %sender_node_id, "Identified incoming TLS connection from node");

    // Update the gossip state with the NodeId for this peer
    // This is critical for bidirectional TLS connections
    {
        let node_id =
            crate::migration::migrate_peer_id_to_node_id(&crate::PeerId::new(&sender_node_id)).ok();
        if let Some(node_id) = node_id {
            registry
                .add_peer_with_node_id(peer_addr, Some(node_id))
                .await;
            debug!(peer_addr = %peer_addr, "Updated gossip state with NodeId for incoming TLS connection");
        }
    }

    // Process the initial message with correlation ID if present
    match msg_result {
        Ok(MessageReadResult::Gossip(msg, _correlation_id)) => {
            // For ActorMessage with correlation_id, ensure it's set
            let msg_to_handle = if let RegistryMessage::ActorMessage {
                actor_id,
                type_hash,
                payload,
                correlation_id: _,
            } = msg
            {
                // Create a new ActorMessage with the correlation_id from the Ask envelope
                RegistryMessage::ActorMessage {
                    actor_id,
                    type_hash,
                    payload,
                    correlation_id: initial_correlation_id,
                }
            } else {
                msg
            };

            if let Err(e) =
                handle_incoming_message(registry.clone(), peer_addr, msg_to_handle).await
            {
                warn!(error = %e, "Failed to process initial TLS message");
            }
        }
        Ok(MessageReadResult::Actor {
            msg_type,
            correlation_id,
            actor_id,
            type_hash,
            payload,
        }) => {
            // Handle initial actor message directly
            if let Some(handler) = &*registry.actor_message_handler.lock().await {
                let actor_id_str = actor_id.to_string();
                let correlation = if msg_type == crate::MessageType::ActorAsk as u8 {
                    Some(correlation_id)
                } else {
                    None
                };
                let _ = handler
                    .handle_actor_message(&actor_id_str, type_hash, &payload, correlation)
                    .await;
            }
        }
        Ok(MessageReadResult::Streaming {
            msg_type,
            stream_header,
            chunk_data,
        }) => {
            // Handle initial streaming message
            match msg_type {
                msg_type if msg_type == crate::MessageType::StreamStart as u8 => {
                    if let Err(e) = streaming_state.start_stream(stream_header) {
                        warn!(error = %e, "Failed to start streaming for stream_id={}", stream_header.stream_id);
                    }
                }
                msg_type if msg_type == crate::MessageType::StreamData as u8 => {
                    // Data chunk - this should not be the first message typically, but handle it
                    if let Err(e) = streaming_state.start_stream(stream_header) {
                        debug!(error = %e, "Auto-starting stream for data chunk: stream_id={}", stream_header.stream_id);
                    }
                    if let Ok(Some(complete_data)) =
                        streaming_state.add_chunk(stream_header, chunk_data)
                    {
                        // Complete message assembled - route to actor
                        if let Some(handler) = &*registry.actor_message_handler.lock().await {
                            let actor_id_str = stream_header.actor_id.to_string();
                            let _ = handler
                                .handle_actor_message(
                                    &actor_id_str,
                                    stream_header.type_hash,
                                    &complete_data,
                                    None,
                                )
                                .await;
                        }
                    }
                }
                _ => {
                    warn!(
                        "Unexpected streaming message type as initial message: 0x{:02x}",
                        msg_type
                    );
                }
            }
        }
        Err(e) => {
            warn!(error = %e, "Failed to read initial message - connection will be closed");
            return None;
        }
    }

    // Add the TLS writer to the connection pool for bidirectional communication
    {
        use std::pin::Pin;
        use tokio::io::AsyncWrite;

        // Box the writer as a trait object for heterogeneous storage
        let boxed_writer: Pin<Box<dyn AsyncWrite + Send>> = Box::pin(writer);

        // Create a LockFreeStreamHandle for the TLS writer
        let stream_handle = Arc::new(crate::connection_pool::LockFreeStreamHandle::new(
            boxed_writer,
            peer_addr,
            crate::connection_pool::ChannelId::Global,
            crate::connection_pool::BufferConfig::default(), // Use BufferConfig with 1MB buffer
        ));

        // Create a connection with the TLS stream handle
        let mut connection = crate::connection_pool::LockFreeConnection::new(peer_addr);
        connection.stream_handle = Some(stream_handle);
        connection.set_state(crate::connection_pool::ConnectionState::Connected);
        connection.update_last_used();

        // Add the connection to the pool by node ID
        let connection_arc = Arc::new(connection);
        let pool = registry.connection_pool.lock().await;

        // Add the connection using the sender's node ID
        pool.add_connection_by_node_id(sender_node_id.clone(), peer_addr, connection_arc);

        debug!(node_id = %sender_node_id, peer_addr = %peer_addr, 
              "Added incoming TLS connection to pool for bidirectional communication");
    }

    // Continue reading messages from the TLS stream
    // Note: writer has been moved to the connection pool, so we only have the reader
    loop {
        match read_message_from_tls_reader(&mut reader, max_message_size).await {
            Ok(MessageReadResult::Gossip(msg, correlation_id)) => {
                // For ActorMessage with correlation_id from Ask envelope, ensure it's set
                let msg_to_handle = if let RegistryMessage::ActorMessage {
                    actor_id,
                    type_hash,
                    payload,
                    correlation_id: _,
                } = msg
                {
                    // Create a new ActorMessage with the correlation_id from the Ask envelope
                    RegistryMessage::ActorMessage {
                        actor_id,
                        type_hash,
                        payload,
                        correlation_id,
                    }
                } else {
                    msg
                };

                if let Err(e) =
                    handle_incoming_message(registry.clone(), peer_addr, msg_to_handle).await
                {
                    warn!(error = %e, "Failed to process TLS message");
                }
            }
            Ok(MessageReadResult::Actor {
                msg_type,
                correlation_id,
                actor_id,
                type_hash,
                payload,
            }) => {
                // Handle actor message directly
                // Call the actor message handler if available
                if let Some(handler) = &*registry.actor_message_handler.lock().await {
                    let actor_id_str = actor_id.to_string();
                    let correlation = if msg_type == crate::MessageType::ActorAsk as u8 {
                        Some(correlation_id)
                    } else {
                        None
                    };
                    let _ = handler
                        .handle_actor_message(&actor_id_str, type_hash, &payload, correlation)
                        .await;
                }
            }
            Ok(MessageReadResult::Streaming {
                msg_type,
                stream_header,
                chunk_data,
            }) => {
                // Handle streaming messages
                match msg_type {
                    msg_type if msg_type == crate::MessageType::StreamStart as u8 => {
                        if let Err(e) = streaming_state.start_stream(stream_header) {
                            warn!(error = %e, "Failed to start streaming for stream_id={}", stream_header.stream_id);
                        }
                    }
                    msg_type if msg_type == crate::MessageType::StreamData as u8 => {
                        if let Ok(Some(complete_data)) =
                            streaming_state.add_chunk(stream_header, chunk_data)
                        {
                            // Complete message assembled - route to actor
                            if let Some(handler) = &*registry.actor_message_handler.lock().await {
                                let actor_id_str = stream_header.actor_id.to_string();
                                let _ = handler
                                    .handle_actor_message(
                                        &actor_id_str,
                                        stream_header.type_hash,
                                        &complete_data,
                                        None,
                                    )
                                    .await;
                            }
                        }
                    }
                    msg_type if msg_type == crate::MessageType::StreamEnd as u8 => {
                        if let Ok(Some(complete_data)) =
                            streaming_state.finalize_stream(stream_header.stream_id)
                        {
                            // Complete message assembled - route to actor
                            if let Some(handler) = &*registry.actor_message_handler.lock().await {
                                let actor_id_str = stream_header.actor_id.to_string();
                                let _ = handler
                                    .handle_actor_message(
                                        &actor_id_str,
                                        stream_header.type_hash,
                                        &complete_data,
                                        None,
                                    )
                                    .await;
                            }
                        }
                    }
                    _ => {
                        warn!("Unknown streaming message type: 0x{:02x}", msg_type);
                    }
                }
            }
            Err(e) => {
                debug!(error = %e, "TLS connection closed or error reading message");
                break;
            }
        }
    }

    Some(sender_node_id)
}

/// Result type for message reading that can handle gossip, actor, and streaming messages
enum MessageReadResult {
    Gossip(RegistryMessage, Option<u16>),
    Actor {
        msg_type: u8,
        correlation_id: u16,
        actor_id: u64,
        type_hash: u32,
        payload: Vec<u8>,
    },
    Streaming {
        msg_type: u8,
        stream_header: crate::StreamHeader,
        chunk_data: Vec<u8>,
    },
}

/// Read a message from a TLS reader
async fn read_message_from_tls_reader<R>(
    reader: &mut R,
    max_message_size: usize,
) -> Result<MessageReadResult>
where
    R: AsyncReadExt + Unpin,
{
    // Read the message length (4 bytes)
    let mut len_buf = [0u8; 4];
    reader.read_exact(&mut len_buf).await?;
    let msg_len = u32::from_be_bytes(len_buf) as usize;

    if msg_len > max_message_size {
        return Err(crate::GossipError::MessageTooLarge {
            size: msg_len,
            max: max_message_size,
        });
    }

    // Read the message data - allocate with proper alignment for rkyv
    // Use Vec::with_capacity to ensure proper allocation alignment
    let mut msg_buf = Vec::with_capacity(msg_len);
    msg_buf.resize(msg_len, 0);
    reader.read_exact(&mut msg_buf).await?;

    // Check if this is an Ask message with envelope
    if msg_len >= 8 && msg_buf[0] == crate::MessageType::Ask as u8 {
        // This is an Ask message with envelope format:
        // [type:1][correlation_id:2][reserved:5][payload:N]

        // Extract correlation ID (bytes 1-2)
        let correlation_id = u16::from_be_bytes([msg_buf[1], msg_buf[2]]);

        // The actual RegistryMessage starts at byte 8
        // Create a properly aligned buffer for the payload
        let payload_len = msg_len - 8;
        let mut aligned_payload = Vec::with_capacity(payload_len);
        aligned_payload.extend_from_slice(&msg_buf[8..]);

        // Deserialize the RegistryMessage from the aligned payload
        let msg: RegistryMessage =
            rkyv::from_bytes::<RegistryMessage, rkyv::rancor::Error>(&aligned_payload)?;

        debug!(
            correlation_id = correlation_id,
            "Received Ask message with correlation ID"
        );

        Ok(MessageReadResult::Gossip(msg, Some(correlation_id)))
    } else {
        // Check if this is a Gossip message with type prefix
        if msg_len >= 1 {
            let first_byte = msg_buf[0];
            // Check if it's a known message type
            if let Some(msg_type) = crate::MessageType::from_byte(first_byte) {
                match msg_type {
                    crate::MessageType::Gossip => {
                        // This is a gossip message with type prefix, skip the type byte
                        if msg_buf.len() > 1 {
                            // Create a properly aligned buffer for the payload
                            let payload_len = msg_len - 1;
                            let mut aligned_payload = Vec::with_capacity(payload_len);
                            aligned_payload.extend_from_slice(&msg_buf[1..]);
                            let msg: RegistryMessage =
                                rkyv::from_bytes::<RegistryMessage, rkyv::rancor::Error>(
                                    &aligned_payload,
                                )?;
                            return Ok(MessageReadResult::Gossip(msg, None));
                        }
                    }
                    crate::MessageType::ActorTell | crate::MessageType::ActorAsk => {
                        // This is an actor message with envelope format:
                        // [type:1][correlation_id:2][reserved:5][actor_id:8][type_hash:4][payload_len:4][payload:N]
                        if msg_buf.len() < 24 {
                            // Need at least 24 bytes for header
                            return Err(crate::GossipError::Network(std::io::Error::new(
                                std::io::ErrorKind::InvalidData,
                                "Actor message too small",
                            )));
                        }

                        // Parse the actor message envelope
                        let msg_type_byte = msg_buf[0];
                        let correlation_id = u16::from_be_bytes([msg_buf[1], msg_buf[2]]);
                        // Skip reserved bytes [3..8]
                        let actor_id = u64::from_be_bytes(msg_buf[8..16].try_into().unwrap());
                        let type_hash = u32::from_be_bytes(msg_buf[16..20].try_into().unwrap());
                        let payload_len =
                            u32::from_be_bytes(msg_buf[20..24].try_into().unwrap()) as usize;

                        if msg_buf.len() < 24 + payload_len {
                            return Err(crate::GossipError::Network(std::io::Error::new(
                                std::io::ErrorKind::InvalidData,
                                "Actor message payload incomplete",
                            )));
                        }

                        let payload = msg_buf[24..24 + payload_len].to_vec();

                        return Ok(MessageReadResult::Actor {
                            msg_type: msg_type_byte,
                            correlation_id,
                            actor_id,
                            type_hash,
                            payload,
                        });
                    }
                    crate::MessageType::StreamStart
                    | crate::MessageType::StreamData
                    | crate::MessageType::StreamEnd => {
                        // Handle streaming messages
                        // Message format: [type:1][correlation_id:2][reserved:5][stream_header:36][chunk_data:N]
                        if msg_buf.len() < 8 + crate::StreamHeader::SERIALIZED_SIZE {
                            return Err(crate::GossipError::Network(std::io::Error::new(
                                std::io::ErrorKind::InvalidData,
                                "Streaming message header incomplete",
                            )));
                        }

                        // Parse the stream header (36 bytes starting at offset 8)
                        let header_bytes = &msg_buf[8..8 + crate::StreamHeader::SERIALIZED_SIZE];
                        let stream_header = crate::StreamHeader::from_bytes(header_bytes)
                            .ok_or_else(|| {
                                crate::GossipError::Network(std::io::Error::new(
                                    std::io::ErrorKind::InvalidData,
                                    "Invalid stream header",
                                ))
                            })?;

                        // Extract chunk data (everything after the header)
                        let chunk_data = if msg_buf.len() > 8 + crate::StreamHeader::SERIALIZED_SIZE
                        {
                            msg_buf[8 + crate::StreamHeader::SERIALIZED_SIZE..].to_vec()
                        } else {
                            Vec::new()
                        };

                        return Ok(MessageReadResult::Streaming {
                            msg_type: first_byte,
                            stream_header,
                            chunk_data,
                        });
                    }
                    _ => {
                        // This is not a recognized message type, return a custom error
                        return Err(crate::GossipError::Network(std::io::Error::new(
                            std::io::ErrorKind::InvalidData,
                            format!("Unknown message type: 0x{:02x}", first_byte),
                        )));
                    }
                }
            }
        }

        // Try to deserialize as regular gossip message (backwards compatibility)
        // Already have msg_buf allocated with proper alignment from above
        let msg: RegistryMessage =
            rkyv::from_bytes::<RegistryMessage, rkyv::rancor::Error>(&msg_buf)?;
        Ok(MessageReadResult::Gossip(msg, None))
    }
}

/// Handle an incoming direct TCP connection - processes all messages and manages the connection pool
async fn handle_incoming_connection_direct_tcp(
    mut reader: tokio::net::tcp::OwnedReadHalf,
    writer: tokio::net::tcp::OwnedWriteHalf,
    peer_addr: SocketAddr,
    registry: Arc<GossipRegistry>,
    registry_weak: Option<std::sync::Weak<GossipRegistry>>,
) -> Option<String> {
    // Returns node ID instead of address
    let max_message_size = registry.config.max_message_size;

    // First, read the initial message to identify the sender
    let msg_result = read_message_from_reader(&mut reader, max_message_size).await;

    let sender_node_id = match &msg_result {
        Ok(msg) => match msg {
            RegistryMessage::DeltaGossip { delta } => delta.sender_peer_id.as_str().to_string(),
            RegistryMessage::FullSync { sender_peer_id, .. } => sender_peer_id.as_str().to_string(),
            RegistryMessage::FullSyncRequest { sender_peer_id, .. } => {
                sender_peer_id.as_str().to_string()
            }
            RegistryMessage::FullSyncResponse { sender_peer_id, .. } => {
                sender_peer_id.as_str().to_string()
            }
            RegistryMessage::DeltaGossipResponse { delta } => {
                delta.sender_peer_id.as_str().to_string()
            }
            RegistryMessage::PeerHealthQuery { sender, .. } => sender.as_str().to_string(),
            RegistryMessage::PeerHealthReport { reporter, .. } => reporter.as_str().to_string(),
            RegistryMessage::ActorMessage { actor_id, .. } => {
                // For actor messages, we can't determine the sender node from the message itself
                // Use a default identifier or extract from actor_id if it contains node info
                format!("actor_message_sender_{}", actor_id)
            }
            RegistryMessage::ImmediateAck { actor_name, .. } => {
                // For ACK messages, use actor name as identifier
                format!("ack_sender_{}", actor_name)
            }
        },
        Err(err) => {
            warn!(error = %err, "failed to read initial message from incoming connection");
            return None;
        }
    };

    debug!(peer_addr = %peer_addr, node_id = %sender_node_id, "Identified incoming connection from node");

    // Add this connection to the pool FIRST - LOCK-FREE
    // This must happen BEFORE processing messages so responses can be sent
    {
        let pool = registry.connection_pool.lock().await;

        debug!(
            "INCOMING CONNECTION: About to add connection to pool - node_id={}, peer_addr={}",
            sender_node_id, peer_addr
        );

        // Check if this node is in our configured peers by checking node mappings
        let sender_peer_id = crate::PeerId::new(&sender_node_id);
        let is_configured_peer = pool.peer_id_to_addr.contains_key(&sender_peer_id);
        debug!(
            "INCOMING CONNECTION: Is {} a configured peer? {}",
            sender_node_id, is_configured_peer
        );

        // Validate peer ID - reject unknown peers
        if !is_configured_peer {
            error!(
                "REJECTED: Unknown peer '{}' attempted to connect from {}. Connection dropped.",
                sender_node_id, peer_addr
            );
            return None;
        }

        // Create the connection
        // Stream handle uses the actual TCP peer address
        let stream_handle = Arc::new(LockFreeStreamHandle::new(
            writer,
            peer_addr, // Use actual TCP peer address for the stream
            ChannelId::Global,
            crate::connection_pool::BufferConfig::default(), // Use BufferConfig with 1MB buffer
        ));

        // Get the shared correlation tracker for this peer
        let sender_peer_id = crate::PeerId::new(&sender_node_id);
        let correlation_tracker = pool.get_or_create_correlation_tracker(&sender_peer_id);

        // Create connection - we'll just use peer_addr for now
        let mut conn = LockFreeConnection::new(peer_addr);
        conn.stream_handle = Some(stream_handle);
        conn.set_state(ConnectionState::Connected);
        conn.update_last_used();
        conn.correlation = Some(correlation_tracker);

        debug!(
            "INCOMING CONNECTION: Using shared correlation tracker for node '{}'",
            sender_node_id
        );

        let connection_arc = Arc::new(conn);

        // Add connection indexed by node ID
        if pool.add_connection_by_node_id(sender_node_id.clone(), peer_addr, connection_arc) {
            info!(node_id = %sender_node_id, peer_addr = %peer_addr,
                  "Added incoming connection to pool for bidirectional use");

            // NOTE: We do NOT update the node address mapping here because peer_addr is the ephemeral
            // source port of the incoming connection, not the bind address where the peer listens.
            // The proper bind address should already be configured through gossip or initial peer setup.

            // Debug: verify it was actually added
            debug!(
                "INCOMING CONNECTION: Pool now has {} connections",
                pool.connection_count()
            );

            // Verify we can retrieve it by node ID
            if let Some(_conn) = pool.get_connection_by_node_id(&sender_node_id) {
                debug!(
                    "INCOMING CONNECTION: Verified connection is retrievable by node ID '{}'",
                    sender_node_id
                );
            } else {
                error!("INCOMING CONNECTION: Connection was added but cannot be retrieved by node ID '{}'!", sender_node_id);
            }
        } else {
            warn!(node_id = %sender_node_id, "Failed to add connection to pool");
            return None;
        }
    }

    // NOW process the first message - connection is ready for responses
    if let Ok(msg) = msg_result {
        if let Err(e) = handle_incoming_message(registry.clone(), peer_addr, msg).await {
            warn!(error = %e, "failed to handle initial message");
        }
    }

    // If this is a configured peer, establish outbound connection immediately
    {
        let pool = registry.connection_pool.lock().await;
        // Check if we were expecting this peer (it's in our bootstrap list)
        let sender_peer_id = crate::PeerId::new(&sender_node_id);
        let should_connect_back = pool.peer_id_to_addr.contains_key(&sender_peer_id);
        drop(pool); // Release lock before connecting

        if should_connect_back {
            debug!(
                "INCOMING CONNECTION: {} is a configured peer, establishing outbound connection",
                sender_node_id
            );
            let registry_for_connect = registry.clone();
            let node_id_clone = sender_node_id.clone();

            tokio::spawn(async move {
                // Give the incoming connection a moment to stabilize
                tokio::time::sleep(Duration::from_millis(50)).await;

                // Now establish our outbound connection BY NODE ID
                let mut pool = registry_for_connect.connection_pool.lock().await;
                match pool.get_connection_to_node(&node_id_clone).await {
                    Ok(_) => {
                        debug!(
                            "Successfully established outbound connection to node '{}'",
                            node_id_clone
                        );
                    }
                    Err(e) => {
                        warn!(
                            "Failed to establish outbound connection to node '{}': {}",
                            node_id_clone, e
                        );
                        // Even if we can't connect back, we can still use the incoming connection
                        // The incoming connection is already in the pool and can be used bidirectionally
                        info!("Will use incoming connection from node '{}' for bidirectional communication", node_id_clone);
                    }
                }
            });
        }
    }

    // Trigger an immediate gossip round to establish bidirectional communication quickly
    {
        let registry_clone = registry.clone();
        let sender_node_id_clone = sender_node_id.clone();
        tokio::spawn(async move {
            debug!(node = %sender_node_id_clone, "Triggering immediate gossip round after incoming connection");

            // Prepare and execute a gossip round
            match registry_clone.prepare_gossip_round().await {
                Ok(tasks) => {
                    if !tasks.is_empty() {
                        // Execute all gossip tasks
                        let mut futures = Vec::new();

                        for task in tasks {
                            let registry_for_task = registry_clone.clone();
                            let peer_addr = task.peer_addr;
                            let sent_sequence = task.current_sequence;
                            let future = tokio::spawn(async move {
                                let outcome =
                                    send_gossip_message_zero_copy(task, registry_for_task).await;
                                GossipResult {
                                    peer_addr,
                                    sent_sequence,
                                    outcome: outcome.map(|_| None),
                                }
                            });
                            futures.push(future);
                        }

                        // Wait for all gossip operations to complete
                        let mut results = Vec::new();
                        for future in futures {
                            match future.await {
                                Ok(result) => results.push(result),
                                Err(err) => {
                                    warn!(error = %err, "gossip task panicked");
                                }
                            }
                        }

                        // Apply the results
                        registry_clone.apply_gossip_results(results).await;
                        debug!(node = %sender_node_id_clone, "Completed immediate gossip round after incoming connection");
                    }
                }
                Err(err) => {
                    warn!(error = %err, node = %sender_node_id_clone, "Failed to trigger immediate gossip round");
                }
            }
        });
    }

    // TODO: Update failure tracking to use node IDs instead of addresses
    // For now, we'll skip the failure reset logic since we're transitioning to node-based tracking

    // Now continue with the persistent connection reader for the rest of the connection
    // Note: writer was already used to create the connection, so we pass None
    crate::connection_pool::handle_persistent_connection_reader(
        reader,
        None,
        peer_addr, // Use the actual peer address for the reader
        registry_weak,
    )
    .await;

    // Return the node ID for proper cleanup
    Some(sender_node_id)
}

async fn read_message(stream: &mut TcpStream, max_size: usize) -> Result<RegistryMessage> {
    // Read length header
    let mut len_buf = [0u8; 4];
    stream.read_exact(&mut len_buf).await?;
    let len = u32::from_be_bytes(len_buf) as usize;

    // Check message size
    if len > max_size {
        return Err(GossipError::MessageTooLarge {
            size: len,
            max: max_size,
        });
    }

    // Read message data
    let mut data = vec![0u8; len];
    stream.read_exact(&mut data).await?;

    let msg = rkyv::from_bytes::<RegistryMessage, rkyv::rancor::Error>(&data)?;
    Ok(msg)
}

async fn read_message_from_reader(
    reader: &mut tokio::net::tcp::OwnedReadHalf,
    max_size: usize,
) -> Result<RegistryMessage> {
    // Read length header
    let mut len_buf = [0u8; 4];
    reader.read_exact(&mut len_buf).await?;
    let len = u32::from_be_bytes(len_buf) as usize;

    // TEMPORARY: Skip message size check to allow large messages
    // TODO: Implement streaming or chunking for very large messages
    if len > max_size {
        warn!(
            "Message size {} exceeds max size {}, but allowing it temporarily",
            len, max_size
        );
    }

    // Read message data - ensure proper alignment for rkyv
    let mut data = Vec::with_capacity(len);
    data.resize(len, 0);
    reader.read_exact(&mut data).await?;

    let msg = rkyv::from_bytes::<RegistryMessage, rkyv::rancor::Error>(&data)?;
    Ok(msg)
}

/// Bootstrap by connecting to known peers and doing initial gossip with vector clocks
#[instrument(skip(_registry))]
async fn bootstrap_peers(_registry: Arc<GossipRegistry>) -> Result<()> {
    // Bootstrap disabled for now - using explicit connect_to_peer() instead
    Ok(())
    /*
    let bind_addr = registry.bind_addr;

    // Get configured node IDs to bootstrap
    let node_ids_and_addrs = {
        let pool = registry.connection_pool.lock().await;
        pool.peer_id_to_addr.iter()
            .map(|entry| (entry.key().clone(), *entry.value()))
            .collect::<Vec<(String, SocketAddr)>>()
    };

    info!(
        peer_count = node_ids_and_addrs.len(),
        nodes = ?node_ids_and_addrs.iter().map(|(id, _)| id).collect::<Vec<_>>(),
        "starting bootstrap with vector clock support"
    );

    for (node_id, peer_addr) in node_ids_and_addrs {
        info!(node_id = %node_id, addr = %peer_addr, "Bootstrapping to node");
        // Always use full sync for bootstrap
        let gossip_msg = {
            let (local_actors, known_actors) = {
                let actor_state = registry.actor_state.read().await;
                (
                    actor_state.local_actors.clone(),
                    actor_state.known_actors.clone(),
                )
            };

            let sequence = {
                let gossip_state = registry.gossip_state.lock().await;
                gossip_state.gossip_sequence
            };

            RegistryMessage::FullSync {
                local_actors: local_actors.into_iter().map(|(k, v)| (k, v)).collect(),
                known_actors: known_actors.into_iter().map(|(k, v)| (k, v)).collect(),
                sender_addr: bind_addr.to_string(), // This is the listening address
                sender_peer_id: registry.peer_id.clone(), // Use peer ID
                sequence,
                wall_clock_time: current_timestamp(),
            }
        };

        // Send bootstrap message
        match rkyv::to_bytes::<rkyv::rancor::Error>(&gossip_msg) {
            Ok(data) => {
                // Get connection and create message buffer
                let (conn, buffer) = {
                    let mut pool = registry.connection_pool.lock().await;

                    info!(node_id = %node_id, "BOOTSTRAP: Attempting to get connection for bootstrap");

                    // Get connection by node ID
                    let conn = match pool.get_connection_to_node(&node_id).await {
                        Ok(c) => {
                            info!(node_id = %node_id, "BOOTSTRAP: Successfully got connection");
                            info!(node_id = %node_id, "BOOTSTRAP: Pool now has {} connections after get_connection", pool.connection_count());
                            c
                        }
                        Err(e) => {
                            // During startup, connection refused is expected if peer's server isn't ready yet
                            // This will be retried by the bootstrap retry logic
                            match &e {
                                GossipError::Network(io_err)
                                    if io_err.kind() == std::io::ErrorKind::ConnectionRefused =>
                                {
                                    info!(node_id = %node_id, addr = %peer_addr, "BOOTSTRAP: Node not ready yet (connection refused)");
                                }
                                _ => {
                                    warn!(node_id = %node_id, error = %e, "BOOTSTRAP: Failed to connect to bootstrap node");
                                }
                            }
                            continue;
                        }
                    };

                    // Create message buffer with length header using buffer pool
                    let buffer = pool.create_message_buffer(&data);

                    (conn, buffer)
                };

                debug!(peer = %peer, "connected to bootstrap peer");

                if let Err(err) = conn.send_data(buffer).await {
                    warn!(peer = %peer, error = %err, "failed to send bootstrap gossip");
                    continue;
                }

                debug!(peer = %peer, "sent bootstrap message through persistent connection");
                // Note: With persistent connections, we'll receive the response through
                // the incoming message handler, not here
            }
            Err(err) => {
                warn!(peer = %peer, error = %err, "failed to serialize bootstrap message");
            }
        }
    }

    info!("bootstrap completed with vector clock support");

    // Trigger an immediate gossip round after bootstrap to establish connections quickly
    tokio::spawn(async move {
        debug!("Triggering immediate gossip round after bootstrap");

        // Prepare and execute a gossip round
        match registry.prepare_gossip_round().await {
            Ok(tasks) => {
                if !tasks.is_empty() {
                    // Execute all gossip tasks
                    let mut futures = Vec::new();

                    for task in tasks {
                        let registry_for_task = registry.clone();
                        let peer_addr = task.peer_addr;
                        let sent_sequence = task.current_sequence;
                        let future = tokio::spawn(async move {
                            let outcome = send_gossip_message_zero_copy(task, registry_for_task).await;
                            GossipResult {
                                peer_addr,
                                sent_sequence,
                                outcome: outcome.map(|_| None),
                            }
                        });
                        futures.push(future);
                    }

                    // Wait for all gossip operations to complete
                    let mut results = Vec::new();
                    for future in futures {
                        match future.await {
                            Ok(result) => results.push(result),
                            Err(err) => {
                                warn!(error = %err, "gossip task panicked");
                            }
                        }
                    }

                    // Apply the results
                    registry.apply_gossip_results(results).await;
                    debug!("Completed immediate gossip round after bootstrap");
                }
            }
            Err(err) => {
                warn!(error = %err, "Failed to trigger immediate gossip round after bootstrap");
            }
        }
    });

    Ok(())
    */
}

/// Send a gossip message through persistent connection with zero-copy optimizations
async fn send_gossip_message_persistent(
    task: GossipTask,
    registry: Arc<GossipRegistry>,
) -> Result<()> {
    // Pre-serialize the message (this could be optimized further with message pooling)
    let data = rkyv::to_bytes::<rkyv::rancor::Error>(&task.message)?;

    // Get persistent connection with minimal lock time
    let conn = {
        let mut pool = registry.connection_pool.lock().await;
        pool.get_connection(task.peer_addr).await?
    };

    // Use zero-copy tell() method for maximum performance
    conn.tell(data.as_slice()).await?;
    Ok(())
}

/// Zero-copy gossip message sender - eliminates bottlenecks in serialization and connection handling
async fn send_gossip_message_zero_copy(
    mut task: GossipTask,
    registry: Arc<GossipRegistry>,
) -> Result<()> {
    // Check if this is a retry attempt
    let is_retry = {
        let gossip_state = registry.gossip_state.lock().await;
        gossip_state
            .peers
            .get(&task.peer_addr)
            .map(|p| p.failures > 0)
            .unwrap_or(false)
    };

    if is_retry {
        info!(
            peer = %task.peer_addr,
            "üîÑ GOSSIP RETRY: Attempting to reconnect to previously failed peer"
        );
    }

    // Get connection with minimal lock contention
    let conn = {
        let mut pool = registry.connection_pool.lock().await;
        debug!(
            "GOSSIP: Pool has {} connections before get_connection",
            pool.connection_count()
        );
        match pool.get_connection(task.peer_addr).await {
            Ok(conn) => {
                if is_retry {
                    info!(
                        peer = %task.peer_addr,
                        "‚úÖ GOSSIP RETRY: Successfully reconnected to peer"
                    );
                }
                conn
            }
            Err(e) => {
                if is_retry {
                    info!(
                        peer = %task.peer_addr,
                        error = %e,
                        "‚ùå GOSSIP RETRY: Failed to reconnect to peer"
                    );
                }
                return Err(e);
            }
        }
    };

    // CRITICAL: Set precise timing RIGHT BEFORE TCP write to exclude all scheduling delays
    // Update wall_clock_time in delta changes to current time for accurate propagation measurement
    let _current_time_secs = crate::current_timestamp();
    let current_time_nanos = crate::current_timestamp_nanos();

    // Debug: Check if there's a delay in the task creation vs sending
    match &task.message {
        crate::registry::RegistryMessage::DeltaGossip { delta } => {
            for change in &delta.changes {
                match change {
                    crate::registry::RegistryChange::ActorAdded { location, .. } => {
                        let creation_time_nanos = location.wall_clock_time as u128 * 1_000_000_000;
                        let delay_nanos = current_time_nanos as u128 - creation_time_nanos;
                        let _delay_ms = delay_nanos as f64 / 1_000_000.0;
                        // eprintln!("üîç DELTA_SEND_DELAY: {}ms between delta creation and sending", delay_ms);
                    }
                    _ => {}
                }
            }
        }
        _ => {}
    }

    match &mut task.message {
        crate::registry::RegistryMessage::DeltaGossip { delta } => {
            delta.precise_timing_nanos = current_time_nanos;
            // Update wall_clock_time in all changes to current time for accurate propagation measurement
            for change in &mut delta.changes {
                match change {
                    crate::registry::RegistryChange::ActorAdded { location, .. } => {
                        // Set wall_clock_time to nanoseconds for consistent timing measurements
                        location.wall_clock_time = (current_time_nanos / 1_000_000_000) as u64;
                    }
                    crate::registry::RegistryChange::ActorRemoved { .. } => {
                        // No wall_clock_time to update
                    }
                }
            }
        }
        crate::registry::RegistryMessage::FullSync { .. } => {
            // Full sync doesn't use precise timing
        }
        _ => {}
    }

    // Serialize the message AFTER updating timing
    let data = rkyv::to_bytes::<rkyv::rancor::Error>(&task.message)?;

    // Create message with Gossip type prefix
    let mut msg_with_type = Vec::with_capacity(1 + data.len());
    msg_with_type.push(crate::MessageType::Gossip as u8);
    msg_with_type.extend_from_slice(&data);

    // Use zero-copy tell() which uses try_send() internally for max performance
    // This completely bypasses async overhead when the channel has capacity
    let tcp_start = std::time::Instant::now();
    conn.tell(msg_with_type.as_slice()).await?;
    let _tcp_elapsed = tcp_start.elapsed();
    // eprintln!("üîç TCP_WRITE_TIME: {:?}", tcp_elapsed);
    Ok(())
}

/// Wait for the server to be ready, then bootstrap
async fn wait_for_server_readiness_and_bootstrap(
    registry: Arc<GossipRegistry>,
    config: GossipConfig,
) -> Result<()> {
    let bind_addr = registry.bind_addr;

    // Wait for server to become ready
    let readiness_timeout = tokio::time::timeout(
        config.bootstrap_readiness_timeout,
        wait_for_server_ready(bind_addr, config.bootstrap_readiness_check_interval),
    );

    match readiness_timeout.await {
        Ok(Ok(())) => {
            // info!("server is ready, starting bootstrap");
        }
        Ok(Err(err)) => {
            warn!(error = %err, "readiness check failed, proceeding with bootstrap anyway");
        }
        Err(_) => {
            warn!("readiness check timeout, proceeding with bootstrap anyway");
        }
    }

    // Add a small initial delay to give peers a chance to start their servers
    // This helps avoid immediate connection failures during startup
    tokio::time::sleep(Duration::from_millis(100)).await;

    // Attempt bootstrap with retries
    for attempt in 1..=config.bootstrap_max_retries {
        match bootstrap_peers(registry.clone()).await {
            Ok(()) => {
                info!(attempt = attempt, "bootstrap completed successfully");
                return Ok(());
            }
            Err(err) => {
                if attempt < config.bootstrap_max_retries {
                    // Use shorter retry delay for initial attempts during startup
                    let retry_delay = if attempt <= 2 {
                        // First two retries: use shorter delay (500ms)
                        Duration::from_millis(500)
                    } else {
                        // Later retries: use configured delay
                        config.bootstrap_retry_delay
                    };

                    warn!(
                        attempt = attempt,
                        max_attempts = config.bootstrap_max_retries,
                        retry_delay_ms = retry_delay.as_millis(),
                        error = %err,
                        "bootstrap attempt failed, retrying"
                    );
                    tokio::time::sleep(retry_delay).await;
                } else {
                    error!(
                        attempt = attempt,
                        error = %err,
                        "bootstrap failed after all retry attempts"
                    );
                    return Err(err);
                }
            }
        }
    }

    Ok(())
}

/// Check if the server is ready by attempting to connect to our own bind address
async fn wait_for_server_ready(bind_addr: SocketAddr, check_interval: Duration) -> Result<()> {
    loop {
        match tokio::time::timeout(Duration::from_millis(500), TcpStream::connect(bind_addr)).await
        {
            Ok(Ok(_stream)) => {
                debug!(bind_addr = %bind_addr, "server readiness confirmed");
                return Ok(());
            }
            Ok(Err(_)) | Err(_) => {
                // Server not ready yet, wait and retry
                tokio::time::sleep(check_interval).await;
            }
        }
    }
}
